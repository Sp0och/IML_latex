\subsection{Regularization in NNs}

Reg. term in loss fct., early stopping, dropout or data augmentation

\textbf{Batch norm.} Normalize unit activations for a laye:
\begin{itemize}
    \item $\mu_s = \frac{1}{|S|}\sum_{i\in S}v_i$
    \item $\sigma_S^2 = \frac{1}{|S|}\sum{i\in S}(v_i - \mu_S)^2$
    \item $\hat{v}_i = \frac{v_i - \mu_S}{\sqrt{\sigma_S^2} + \epsilon}$
    \item Scale and shift: $\bar{v}_i = \gamma\hat{v}_i + \beta$, $\gamma,\beta$ given params.
\end{itemize}
Speeds up and stabilizes training. Solves covariate shift, reduces importance of weight init. Introduces exploding gradients. Mild reg. effect because of "random" batch size.