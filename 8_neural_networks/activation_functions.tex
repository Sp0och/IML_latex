\subsection{Activation Functions}

A neural network with one hidden layer and nonlinear activation functions can approximate every continuous function.

\begin{itemize}
    \item Sigmoid
    \item Relu
    \item Relu Variants
\end{itemize}