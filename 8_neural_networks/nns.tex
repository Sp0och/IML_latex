Backpropagation: Running time grows linearly with num of params in feed forward
momentum and that stuff


Vanishing, exploding gradient. vanishing problem not there for every input

Weight decay reduces complexity

what functions can be approximated at what point. A NN with one hidden layer and a nonlinear act. function can approximate every continuous function

CNNs: need also nonlinear act fcts to approximate nonlin fcts

\input{8_neural_networks/activation_functions.tex}
\input{8_neural_networks/8.i_backpropagation.tex}
\input{8_neural_networks/8.j_vanishing_exploding_gradient.tex}
\input{8_neural_networks/convolutional_nns.tex}
\input{8_neural_networks/regularization.tex}
\input{8_neural_networks/resnet.tex}