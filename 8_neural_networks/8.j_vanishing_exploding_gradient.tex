\subsection{Vanishing / Exploding Gradient}

Potential reasons:
\begin{itemize}
    \item $||\delta^{(i)}|| \rightarrow 0 | \infty$ or $||v^{(i)}|| \rightarrow 0 | \infty$
    \item Relu (no saturation) can help avoid $\delta \rightarrow 0$
    \item Note $\delta$ only depends on $\varphi'$ while v depends on $\varphi$
    \item Use standardized input and / or batch normalization
    \item Weight init. matters as weight opt. is gen. non-convex
\end{itemize}


